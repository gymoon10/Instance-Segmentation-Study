{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model(UNet+ConvLSTM).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g_m0rs96v9hn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "import os\n",
        "# from ConvLSTM import ConvLSTMCell -> ConvLSTM.py\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture (UNet based)"
      ],
      "metadata": {
        "id": "w5tp9Jg3phSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/145318807-8818aa85-6bab-40ec-b9cd-9fe91d3bff7b.png)"
      ],
      "metadata": {
        "id": "nZ0vQQyg2gB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 4개의 이미지를 입력으로 받음 (t-3, t-2, t-1, t)\n",
        "\n",
        "2. Generator1은 각각의 이미지를 입력으로 받아 LSTM1, LSTM2의 입력을 출력 (out1s, out2s)\n",
        "\n",
        "3. LSTM1, LSTM2는 2의 출력을 각각 입력으로 받음\n",
        "\n",
        " - LSTM1은 out1 4개를 입력으로 받아 output1을 출력 \n",
        " - LSTM2는 out2 4개를 입력으로 받아 output2를 출력\n",
        "\n",
        " - output1, 2는 4개 시점의 time information이 모두 고려된 단일 출력\n",
        "\n",
        "4. Generator2는 output1, output2를 입력으로 받아 예측 이미지 생성 (t-2_hat, t-1_hat, t_hat, t+1_hat)"
      ],
      "metadata": {
        "id": "MBhwnj-KsYKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolution"
      ],
      "metadata": {
        "id": "fWosMEjXpnMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input에 2D Convolution 적용\n",
        "# (N, C_in, H, W) -> (N, C_out, H_out, W_out)\n",
        "# Output size = ((W - Kernel size + 2*Padding size) / Strides) + 1\n",
        "\n",
        "# conv 3x3은 zero padding을 사용하여, 연산 후에도 입력의 원본 사이즈를 계속 유지 (파란색 화살표)\n",
        "def conv3x3(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                     stride=1, padding=1, bias=True)  # padding=1 : padding을 줄지 말지 여부 & padding 사이즈 지정\n",
        "\n",
        "# conv 2x2는 zero padding을 사용하지 않기 때문에 연산 후 입력의 사이즈가 감소, 채널 증가 (주황색 화살표)    \n",
        "def conv2x2(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=2,\n",
        "                     stride=2, padding=0, bias=True)  # padding의 default=0 (따로 설정하지 않으면 zero padding 수행 x)\n",
        "\n",
        "def conv1x1(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                     stride=1, padding=0, bias=True)"
      ],
      "metadata": {
        "id": "mQS_xC56pgbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generators"
      ],
      "metadata": {
        "id": "7MaQ5yy-pphH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the inputs of LSTM1 & LSTM2\n",
        "class Generator1(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Generator1, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # 1 - Original Input Images size (128x128 with 16 channels) / input : 4 channels\n",
        "        # 128x128 사이즈 유지\n",
        "        self.conv3_1_1 = conv3x3(in_channels=self.in_channels, out_channels=16)\n",
        "        self.relu1_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_2 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_3 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 2 - 1/2 Size (64x64 with 32 channels) \n",
        "        # 64x64 사이즈 유지\n",
        "        self.conv2_2_1 = conv2x2(in_channels=16, out_channels=32) # 이미지의 사이즈를 감소시키는 대신 depth(channel)가 증가 \n",
        "        self.relu2_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_1 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_2 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 3 - 1/4 Size \n",
        "        # 32x32 사이즈 유지\n",
        "        self.conv2_3_1 = conv2x2(in_channels=32, out_channels=64)   # 이미지의 사이즈를 감소시키는 대신 depth(channel)가 증가 \n",
        "        self.relu3_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        # 16x16 사이즈 유지\n",
        "        self.conv2_4_1 = conv2x2(in_channels=64, out_channels=128)   # 이미지의 사이즈를 감소시키는 대신 depth(channel)가 증가 \n",
        "        self.relu4_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_1 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_2 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1 - Original Input Images (128x128 with 16 channels)\n",
        "        y1_1 = self.conv3_1_1(x)  # out_channels=16\n",
        "        y1_1 = self.relu1_1(y1_1)\n",
        "        y1_2 = self.conv3_1_2(y1_1)  # out_channels=16\n",
        "        y1_2 = self.relu1_2(y1_2)\n",
        "        y1_3 = self.conv3_1_3(y1_2)  # out_channels=16\n",
        "        y1_3 = self.relu1_3(y1_3)  # shape : (None, 16, 128, 128)\n",
        "\n",
        "        # 2 - 1/2 Size (64x64 with 32 channels) \n",
        "        y2_1 = self.conv2_2_1(y1_3)  # out_channels=32 / 여기서 1/2 size로 감소 : ((128-2+2*0) / 2) + 1 = 64\n",
        "        y2_1 = self.relu2_1(y2_1)\n",
        "        y2_2 = self.conv3_2_1(y2_1)  # out_channels=32\n",
        "        y2_2 = self.relu2_2(y2_2)\n",
        "        y2_3 = self.conv3_2_2(y2_2)  # out_channels=32\n",
        "        out1 = self.relu2_3(y2_3)  # Input of LSTM1 / shape : (None, 32, 64, 64)\n",
        "\n",
        "        # 3 - 1/4 Size (32x32 with 64 channels) \n",
        "        y3_1 = self.conv2_3_1(out1)  # out_channels=64 / 여기서 1/4 size로 감소 : ((64-2+2*0) / 2) + 1 = 32\n",
        "        y3_1 = self.relu3_1(y3_1)\n",
        "        y3_2 = self.conv3_3_1(y3_1)  # out_channels=64  \n",
        "        y3_2 = self.relu3_2(y3_2)\n",
        "        y3_3 = self.conv3_3_2(y3_2)  # out_channels=64  \n",
        "        y3_3 = self.relu3_3(y3_3)\n",
        "\n",
        "        # 4 - 1/8 Size (16x16 with 128 channels) \n",
        "        y4_1 = self.conv2_4_1(y3_3)  # out_channels=128\n",
        "        y4_1 = self.relu4_1(y4_1)\n",
        "        y4_2 = self.conv3_4_1(y4_1)  # out_channels=128\n",
        "        y4_2 = self.relu4_2(y4_2)\n",
        "        y4_3 = self.conv3_4_2(y4_2)  # out_channels=128\n",
        "        out2 = self.relu4_3(y4_3)  #  Input of LSTM2\n",
        "\n",
        "        return out1, out2"
      ],
      "metadata": {
        "id": "-8tZL_T22Boa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator2(nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super(Generator2, self).__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.PS = nn.PixelShuffle(2)  # 2배 (None, 32 x 2^2, H, W) -> (None, 32, H x 2, W x 2)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        self.conv3_4_3 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_6 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_4 = conv3x3(in_channels=128, out_channels=256)\n",
        "        self.relu4_7 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 5 - 1/4 Size\n",
        "        self.conv3_5_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu5_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_5_2 = conv3x3(in_channels=64, out_channels=128)\n",
        "        self.relu5_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 6 - 1/2 Size\n",
        "        self.conv3_6_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu6_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_6_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu6_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 7 - Original Input Images size\n",
        "        self.conv3_7_1 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu7_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_7_2 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu7_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv1_7_1 = conv1x1(in_channels=16, out_channels=self.out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):  # x1 : out1 (64x64 with 32 channels), x2 : out2 (16x16 with 128 channels)\n",
        "        # 4 - 1/8 Size\n",
        "        y4_6 = self.conv3_4_3(x2)  # out_channels=128\n",
        "        y4_6 = self.relu4_6(y4_6)\n",
        "        y4_7 = self.conv3_4_4(y4_6)  # out_channels=256\n",
        "        y4_7 = self.relu4_7(y4_7)\n",
        "\n",
        "        # 5 - 1/4 Size\n",
        "        y5_1 = self.PS(y4_7)   # Pixel shuffler / out_channels=64 (초록색 화살표) / shape : (None, 64, 32, 32)\n",
        "        y5_2 = self.conv3_5_1(y5_1)  # out_channels=64\n",
        "        y5_2 = self.relu5_1(y5_2)\n",
        "        y5_3 = self.conv3_5_2(y5_2)  # out_channels=128\n",
        "        y5_3 = self.relu5_2(y5_3)\n",
        "\n",
        "        # 6 - 1/2 Size\n",
        "        y6_1 = self.PS(y5_3)   # Pixel shuffler / out_channels=32 /  shape : (None, 32, 64, 64)\n",
        "        y6_2 = torch.cat((x1, y6_1), 1)  # Concat (32+32 channels) / shape : (None, 32+32, 64, 64)\n",
        "        y6_3 = self.conv3_6_1(y6_2)  # out_channels=64  \n",
        "        y6_3 = self.relu6_1(y6_3)\n",
        "        y6_4 = self.conv3_6_2(y6_3)  # out_channels=64\n",
        "        y6_4 = self.relu6_2(y6_4)\n",
        "\n",
        "         # 7 - Original Input Images size\n",
        "        y7_1 = self.PS(y6_4)  # out_channels=16 / shape : (None, 16, 128, 128)\n",
        "        y7_2 = self.conv3_7_1(y7_1)  # out_channels=16\n",
        "        y7_2 = self.relu7_1(y7_2)\n",
        "        y7_3 = self.conv3_7_2(y7_2)  # out_channels=16\n",
        "        y7_3 = self.relu7_2(y7_3)\n",
        "        out = self.conv1_7_1(y7_3)  # # out_channels=4 / generated images (흰색 화살표)\n",
        "\n",
        "        return out    "
      ],
      "metadata": {
        "id": "i5mDrX2Rpuwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional LSTM (ConvLSTM.py)"
      ],
      "metadata": {
        "id": "CmaTp-I4VYYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/145341328-9d6b9d47-d997-419c-9cea-d2348f2ba01e.png)"
      ],
      "metadata": {
        "id": "GnFYCCTFucE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/145342215-05296e7a-ce7f-46dc-b298-0171abfd7388.png)\n",
        "\n",
        "\n",
        "m=4 (4개 시점의 이미지)\n",
        "\n"
      ],
      "metadata": {
        "id": "KrAFjDcNuyWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KZcRCzAyVRwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channel=sef.input_dim + self.hidden_dim,\n",
        "                              out_channel=4*self.hidden_dim,  # time x channels\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "        \n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state  # shape of input_tensor & h_cur : (None, 32, 64, 64)\n",
        "        # 현 시점의 입력(이미지), 이전 시점의 hidden_state를 입력으로 받음\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis / shape : (None, 32+32, 64, 64)\n",
        "\n",
        "        combined_conv = self.conv(combined)  # shape : (1, 128, 64, 64) / 128=4x32\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)  # shape : (1, 32, 64, 64) \n",
        "        i = torch.sigmoid(cc_i)  # input gate\n",
        "        f = torch.sigmoid(cc_f)  # forget gate\n",
        "        o = torch.sigmoid(cc_o)  # output gate\n",
        "        g = torch.tanh(cc_g)  # update gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next  # shape : (1, 32, 64, 64)\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
      ],
      "metadata": {
        "id": "4poigliPVK4y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model (testblock)"
      ],
      "metadata": {
        "id": "VYNLonq4p3cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  model = testblock(3, 3, 4, 128) - hscnn_main_output4.py\n",
        "class testblock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time, size):\n",
        "        super(testblock, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels  # 3 (RGB)\n",
        "        self.out_channels = out_channels  # 3\n",
        "        self.time = time  # 4 (t-3, t-2, t-1, t 시점의 이미지를 입력으로 받음)\n",
        "        self.size = size  # 128\n",
        "        self.PS = nn.PixelShuffle(2)\n",
        "\n",
        "        self.G1 = Generator1(self.in_channels)  # out1 (32 channels) & out2 (64 channels)\n",
        "        self.G2 = Generator2(self.out_channels)  # generated images (128x128 with 4 channels)\n",
        "\n",
        "        # 2 - 1/2 Size \n",
        "        # Encoder-Decoder of LSTM 1 (input : out1)\n",
        "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=32,\n",
        "                                               hidden_dim=32,\n",
        "                                               kernel_size=(3, 3),\n",
        "                                               bias=True)\n",
        "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=32,\n",
        "                                               hidden_dim=32,\n",
        "                                               kernel_size=(3, 3),\n",
        "                                               bias=True)\n",
        "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=32,\n",
        "                                               hidden_dim=32,\n",
        "                                               kernel_size=(3, 3),\n",
        "                                               bias=True)\n",
        "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=32,\n",
        "                                               hidden_dim=32,\n",
        "                                               kernel_size=(3, 3),\n",
        "                                               bias=True)\n",
        "        \n",
        "        # 4 - 1/8 Size (out2)\n",
        "        # Encoder-Decoder of LSTM 2\n",
        "        self.encoder_1_convlstm_2 = ConvLSTMCell(input_dim=128,\n",
        "                                                 hidden_dim=128,\n",
        "                                                 kernel_size=(3, 3),\n",
        "                                                 bias=True)\n",
        "        self.encoder_2_convlstm_2 = ConvLSTMCell(input_dim=128,\n",
        "                                                 hidden_dim=128,\n",
        "                                                 kernel_size=(3, 3),\n",
        "                                                 bias=True)\n",
        "        self.decoder_1_convlstm_2 = ConvLSTMCell(input_dim=128,\n",
        "                                                 hidden_dim=128,\n",
        "                                                 kernel_size=(3, 3),\n",
        "                                                 bias=True)\n",
        "        self.decoder_2_convlstm_2 = ConvLSTMCell(input_dim=128,\n",
        "                                                 hidden_dim=128,\n",
        "                                                 kernel_size=(3, 3),\n",
        "                                                 bias=True)\n",
        "        \n",
        "        # 2 - 1/2 Size\n",
        "        # Encoder-Decoder\n",
        "        def autoencoder1(self, x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
        "            outputs = []\n",
        "\n",
        "            # encoder\n",
        "            # iteration 과정에서 이전 시점의 h_t를 입력으로 받아 갱신한 후, 다음 시점의 입력으로 제공\n",
        "            for t in range(seq_len):\n",
        "                h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :],  # 특정 시점 t의 out1 / shape : (None, 32, 64, 64)\n",
        "                                                   cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
        "                # shape of h_t : \n",
        "                h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n",
        "                                                     cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
        "\n",
        "            # encoder_vector (Input of decoder, 마지막 시점의 최종 hidden_state)\n",
        "            encoder_vector = h_t2  # 4개 시점의 time information 반영 (compressed)\n",
        "\n",
        "            # decoder\n",
        "            for t in range(seq_len):\n",
        "                h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n",
        "                                                     cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
        "                h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3,\n",
        "                                                     cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
        "                encoder_vector = h_t4\n",
        "\n",
        "                outputs += [h_t4]  # predictions\n",
        "\n",
        "            outputs = torch.stack(outputs, 1)  # decompress (시점의 차원으로 확장하여 tensor를 쌓음)\n",
        "            \n",
        "            return outputs\n",
        "\n",
        "    def autoencoder2(self, x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
        "        outputs = []\n",
        "\n",
        "        # encoder\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.encoder_1_convlstm_2(input_tensor=x[:, t, :, :],\n",
        "                                                 cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
        "            h_t2, c_t2 = self.encoder_2_convlstm_2(input_tensor=h_t,\n",
        "                                                   cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
        "\n",
        "        # encoder_vector\n",
        "        encoder_vector = h_t2  \n",
        "\n",
        "        # decoder\n",
        "        for t in range(seq_len):\n",
        "            h_t3, c_t3 = self.decoder_1_convlstm_2(input_tensor=encoder_vector,\n",
        "                                                   cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
        "            h_t4, c_t4 = self.decoder_2_convlstm_2(input_tensor=h_t3,\n",
        "                                                   cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
        "            encoder_vector = h_t4\n",
        "\n",
        "            outputs += [h_t4]  # predictions\n",
        "        \n",
        "        outputs = torch.stack(outputs, 1)\n",
        "\n",
        "        return outputs   \n",
        "\n",
        "    def forward(self, x1, x2, x3, x4):  # x1, x2, x3, x4 => 이미지 4개 입력 / t-3, t-2, t-1, t 시점의 이미지를 입력으로 받아 t+1시점의 예측 이미지 생성\n",
        "        y1_1, y1_2 = self.G1(x1)  # G1 함수 / 이미지 1의 out1, out2\n",
        "        y2_1, y2_2 = self.G1(x2)  # 이미지 2의 out1, out2\n",
        "        y3_1, y3_2 = self.G1(x3)  # 이미지 3의 out1, out2\n",
        "        y4_1, y4_2 = self.G1(x4)  # 이미지 4의 out1, out2   \n",
        "\n",
        "        # 2 - 1/2 Size (out1s) - shape : (None, 32, 64, 64) / (Batch size, Channels, H, W)\n",
        "        # Encoder-Decoder of LSTM1\n",
        "        y1_1 = y1_1.unsqueeze(1)  # shape : (None, 1, 32, 64, 64)\n",
        "        y2_1 = y2_1.unsqueeze(1)\n",
        "        y3_1 = y3_1.unsqueeze(1)\n",
        "        y4_1 = y4_1.unsqueeze(1)\n",
        "        stack1 = torch.cat((y1_1, y2_1, y3_1, y4_1), dim=1)  # Concat / shape : (None, 4, 32, 64, 64) - (Batch, time, channel, w, h)\n",
        "\n",
        "        b, seq_len, _, h, w = stack1.size() \n",
        "\n",
        "        output1 = self.autoencoder1(stack1, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
        "        output1 = output1.view(-1, 128, 64, 64) # 128=4x32\n",
        "\n",
        "        x1_1_dec = output1[:, 0:32, :, :]  # shape : (None, 32, 64, 64)\n",
        "        x2_1_dec = output1[:, 32:64, :, :]\n",
        "        x3_1_dec = output1[:, 64:96, :, :]\n",
        "        x4_1_dec = output1[:, 96:128, :, :]\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        # Encoder-Decoder of LSTM2\n",
        "        y1_2 = y1_2.unsqueeze(1)\n",
        "        y2_2 = y2_2.unsqueeze(1)\n",
        "        y3_2 = y3_2.unsqueeze(1)\n",
        "        y4_2 = y4_2.unsqueeze(1)\n",
        "        stack2 = torch.cat((y1_2, y2_2, y3_2, y4_2), dim=1)\n",
        "\n",
        "        b_2, seq_len_2, _, h_2, w_2 = stack2.size()\n",
        "\n",
        "        # initialize hidden states\n",
        "        h_t_2, c_t_2 = self.encoder_1_convlstm_2.init_hidden(batch_size=b_2, image_size=(h_2, w_2))\n",
        "        h_t2_2, c_t2_2 = self.encoder_2_convlstm_2.init_hidden(batch_size=b_2, image_size=(h_2, w_2))\n",
        "        h_t3_2, c_t3_2 = self.decoder_1_convlstm_2.init_hidden(batch_size=b_2, image_size=(h_2, w_2))\n",
        "        h_t4_2, c_t4_2 = self.decoder_2_convlstm_2.init_hidden(batch_size=b_2, image_size=(h_2, w_2))\n",
        "\n",
        "        output2 = self.autoencoder2(stack2, seq_len_2, h_t_2, c_t_2, h_t2_2, c_t2_2, h_t3_2, c_t3_2, h_t4_2, c_t4_2)\n",
        "        output2 = output2.view(-1, 512, 16, 16)  # 512 = 4 x 128 (time x channels)\n",
        "\n",
        "        x1_2_dec = output2[:, 0:128, :, :]  # shape : (1, 128, 64, 64)\n",
        "        x2_2_dec = output2[:, 128:256, :, :]\n",
        "        x3_2_dec = output2[:, 256:384, :, :]\n",
        "        x4_2_dec = output2[:, 384:512, :, :]\n",
        "        \n",
        "        # t시점의 입력에 대한 LSTM1, LSTM2의 output들을 입력으로 받아 t+1시점의 예측 이미지 생성 \n",
        "        # 실제로는 out4만 필요, 나머지는 loss 계산용\n",
        "        out1 = self.G2(x1_1_dec, x1_2_dec)\n",
        "        out2 = self.G2(x2_1_dec, x2_2_dec)\n",
        "        out3 = self.G2(x3_1_dec, x3_2_dec)\n",
        "        out4 = self.G2(x4_1_dec, x4_2_dec)\n",
        "\n",
        "        return out1, out2, out3, out4\n"
      ],
      "metadata": {
        "id": "hUR8t_QGCmDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}