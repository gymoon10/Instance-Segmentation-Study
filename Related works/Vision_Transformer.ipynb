{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "참고 : \n",
        "\n",
        "\n",
        "https://github.com/gymoon10/Paper-Review/blob/main/NLP/Attention%20is%20All%20you%20Need%20-%20%EC%84%A4%EB%AA%85%26%EB%85%BC%EB%AC%B8%EC%9D%BD%EA%B8%B0.ipynb\n",
        "\n",
        "\n",
        "https://github.com/gymoon10/Paper-Review/blob/main/NLP/BERT.md"
      ],
      "metadata": {
        "id": "8w26I2w6_BKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjwE7SOG--kQ",
        "outputId": "be73241a-682c-4c44-d8a7-90843c1c794d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "vbJ94HM6-ydQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input image (B, C, H, W)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJJRc3Oj-yf-",
        "outputId": "bdf80b5c-c128-42b4-8d81-8e9865a9285f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/145760178-b6fffe96-8ed8-4525-aacf-d3bb43a9b231.png)"
      ],
      "metadata": {
        "id": "LuoPB6U1X2VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch Embeddings\n",
        "\n",
        "NLP 관점에서 입력 단어에 대한 token embedding\n",
        "\n",
        "입력 이미지를 patch로 나누고 flatten (1차원 벡터로 projection)"
      ],
      "metadata": {
        "id": "FnxsHzN0j3Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input image -> Flattened Patch Sequence (BxCxHxW -> BxNx(P*P*C), P : patch size, N : # of patches H*W/P*P = sequence length)\n",
        "# Linear Embedding (linearly projects the flattened patches into a lower dimensional space)\n",
        "patch_size = 16  # 16 pixels / 총 패치의 개수는 14x14\n",
        "print('x :', x.shape)\n",
        "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
        "print('patches :', patches.shape)  # 8x3x(14*16)x(14*16) -> 8x(14*14)x(16*16*3) / NLP 관점에서 196개의 token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7VT76F_-yio",
        "outputId": "bc7aee1e-b23c-4dd3-9aaf-21f679a8c0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x : torch.Size([1, 3, 224, 224])\n",
            "patches : torch.Size([1, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input image -> Flattened Patch\n",
        "# kernel, stride size를 patch size로 갖는 conv layer 적용\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "emb_size = 768\n",
        "\n",
        "projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),  # NLP 관점에서 입력 단어에 대한 token embedding\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "# 196 : 입력 이미지를 구성하는 196개의 패치\n",
        "# 768 : 각 패치는 768의 embedding 길이를 갖는 1차원 벡터\n",
        "projection(x).shape  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Tv-KIw-dGx",
        "outputId": "78266181-9412-4099-f8b8-0870c90319fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch + Position Embedding + [CLS]\n",
        "emb_size = 768\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "\n",
        "# 입력 이미지를 patch로 변환 후 flatten\n",
        "projected_x = projection(x)\n",
        "print('Projected X shape :', projected_x.shape)\n",
        "\n",
        "# [CLS] 추가 & positional encoding 초기화\n",
        "# 학습되면서 이미지의 전반적인 representation, 공간 정보를 반영할 수 있도록 갱신됨\n",
        "cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape)  # 197=196+1([CLS] 추가로 늘어난 크기)\n",
        "\n",
        "# [CLS]를 배치사이즈의 크기와 맞춰줌 (repeat)\n",
        "batch_size = 1\n",
        "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
        "print('Repeated Cls shape :', cls_tokens.shape)\n",
        "\n",
        "# [CLS], projected_x를 concat\n",
        "cat_x = torch.cat([cls_tokens, projected_x], dim=1)\n",
        "\n",
        "# position encoding을 더해줌\n",
        "cat_x += positions\n",
        "print('output : ', cat_x.shape)  # NLP 관점에서 197개의 embedded token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkHNLLm0A19x",
        "outputId": "71852823-7d4a-4bc7-a281-8b21c9eecb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projected X shape : torch.Size([1, 196, 768])\n",
            "Cls Shape : torch.Size([1, 1, 768]) , Pos Shape : torch.Size([197, 768])\n",
            "Repeated Cls shape : torch.Size([1, 1, 768])\n",
            "output :  torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : BERT\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/145760913-ad8f5a0c-09f1-4196-928c-b102c503de6d.png)\n",
        "\n",
        "* Input : 입력 이미지에 대한 patch\n",
        "\n",
        "* Token Embeddings : projected_x\n",
        "\n",
        "* Position Embeddings : positions\n",
        "\n",
        "\n",
        "Token Embeddings에 Position Embeddings를 더함"
      ],
      "metadata": {
        "id": "4-ir26I3Ztf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤한 값이지만 학습되면서 갱신됨 (learned position embedding is added to the patch representations)\n",
        "# Model learns to encode distance within the image in the similarity of position embeddings (가까운 패치일 수록 유사도가 높은 embedding을 가짐)\n",
        "# 2D image topology를 학습\n",
        "positions  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNMEAhMnFrJk",
        "outputId": "f2565b40-6b62-4cd4-ef0b-10f9d62606ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.6821, -1.7668,  0.5707,  ..., -0.3544,  1.4827,  2.4232],\n",
              "        [-1.4466, -0.9558, -0.0930,  ...,  0.4000, -0.0330,  0.1503],\n",
              "        [-1.8524, -1.4030, -1.2173,  ...,  1.1249, -0.2161,  0.7137],\n",
              "        ...,\n",
              "        [-0.9177,  1.9372, -1.6234,  ..., -0.1032, -0.8573,  1.3289],\n",
              "        [ 0.2576,  0.5573, -1.9590,  ...,  0.2052,  0.7386,  0.2253],\n",
              "        [ 2.0091, -0.3537,  1.5352,  ...,  0.3217,  0.8891, -1.2121]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 과정을 하나의 class로\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e')\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "\n",
        "    def forward(self, x):  # x : Input image\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x) # Input image to flattened patch\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)  # [CLS] 추가\n",
        "        x += self.positions  # position embedding 추가\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "W6LR1O2CBUiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PatchEmbedding()(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ctsJryYIzZM",
        "outputId": "2d62005e-45e8-412d-fdac-0bad3d74d3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patches_embedded = PatchEmbedding()(x)  # NLP의 embedding된 word token 역할 (e.g BERT의 입력)"
      ],
      "metadata": {
        "id": "1AEFtHBrI3i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MHA (Multi Head Attention)\n",
        "\n",
        "\n",
        "Q, K, V는 동일한 텐서로 입력\n",
        "\n",
        "3개의 linear projection을 통해 각각 임베딩되어 여러 개의 head로 나눠진 후 각각 scaled dot product attention 연산 수행 \n",
        "\n",
        "임베딩된 입력 텐서를 입력으로 받아 다시 임베딩 사이즈로 linear projection을 수행하는 layer를 3개 생성. (학습 과정에서 입력 텐서를 Q, K, V로 만드는 layer가 학습됨)"
      ],
      "metadata": {
        "id": "2pWpPTHeG_cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : Attention is all you need\n",
        "\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/145761233-dca75a16-727f-43a3-9018-c379f9e1f95f.png)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/145761782-793c9358-667d-4f0b-9dba-79bec3b3b579.png)"
      ],
      "metadata": {
        "id": "LUMjk27fafSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patches_embedded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHp9K37tmVNv",
        "outputId": "31d1f05c-28b5-4058-cbab-d80cbcb07733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/145762488-0490003c-ffee-460f-9efa-d12c8388beff.png)"
      ],
      "metadata": {
        "id": "BcdrORdkdQfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_size = 768\n",
        "num_heads = 8\n",
        "\n",
        "# embedding된 입력 텐서를 받아 linear projection 수행 \n",
        "keys = nn.Linear(emb_size, emb_size)\n",
        "queries = nn.Linear(emb_size, emb_size)\n",
        "values = nn.Linear(emb_size, emb_size)\n",
        "print(keys, queries, values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGXZm2pIG7hR",
        "outputId": "ef2144af-2c1c-4b42-d1d2-479bad780d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QKV (Linear Projection)\n",
        "queries = rearrange(queries(patches_embedded), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "keys = rearrange(keys(patches_embedded), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "values  = rearrange(values(patches_embedded), \"b n (h d) -> b h n d\", h=num_heads)\n",
        "\n",
        "print('shape :', queries.shape, keys.shape, values.shape)  # (Batch, Heads, Seq_len, Embedding_size), 768=96x8 / 8 : # of heads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb-2-g4RHgpD",
        "outputId": "e5d1a728-0218-42eb-d6f3-03f1143fc8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape : torch.Size([1, 8, 197, 96]) torch.Size([1, 8, 197, 96]) torch.Size([1, 8, 197, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled dot product attention\n",
        "# Shape : (Batch, Heads, Query_len, Key_len)로 동일\n",
        "# Queries * Keys\n",
        "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # 자동으로 transpose\n",
        "print('energy :', energy.shape)\n",
        "\n",
        "# Get Attention Score\n",
        "scaling = emb_size ** (1/2)\n",
        "att = F.softmax(energy, dim=-1) / scaling\n",
        "print('att :', att.shape)\n",
        "\n",
        "# Attention Score * values\n",
        "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "print('out :', out.shape)\n",
        "\n",
        "# Rearrage to emb_size (Concat)\n",
        "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "print('out2 : ', out.shape)  # MHA Output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQpNBWx9Igcm",
        "outputId": "cdf71c5f-35ff-4261-d43e-cab0a10ea180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "energy : torch.Size([1, 8, 197, 197])\n",
            "att : torch.Size([1, 8, 197, 197])\n",
            "out : torch.Size([1, 8, 197, 96])\n",
            "out2 :  torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 과정을 하나의 class로\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # Q,K,V를 하나의 매트릭스에 저장\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        \n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # Q,K,V 분할\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # (Batch, Heads, Query_len, Key_len)\n",
        "\n",
        "        # Attention 연산 수행 시 무시할 정보를 설정\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "  \n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "d7ayQ4baJQeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MultiHeadAttention()(patches_embedded).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C8hXlffaZe7",
        "outputId": "ef3fab04-5787-4f42-87f4-3125e5e78476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residuals"
      ],
      "metadata": {
        "id": "XLTMKuGLb0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "_u9krzu9bKde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP\n",
        "\n",
        "Linear -> GELU -> Dropout -> Linear\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/145762755-a0a938f9-a406-4222-b89e-1c1e925fbd0b.png)"
      ],
      "metadata": {
        "id": "XwnHSaC9dW6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),  # 확장\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),  # 원래대로 축소         \n",
        "        )"
      ],
      "metadata": {
        "id": "6HB8BH7EbzMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block"
      ],
      "metadata": {
        "id": "2grdVN9dd8nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 768,\n",
        "                 drop_p: float = 0.,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))"
      ],
      "metadata": {
        "id": "QnG9_hgedmmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patches_embedded = PatchEmbedding()(x)\n",
        "TransformerEncoderBlock()(patches_embedded).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV94fJ7PeR34",
        "outputId": "5b503709-4aaf-42a3-8c77-74952bd29334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer\n",
        "\n",
        "Transformer Encoder Block을 depth만큼 쌓음"
      ],
      "metadata": {
        "id": "SZnJHLCLet8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
      ],
      "metadata": {
        "id": "_aL31FxZeSBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Head\n",
        "\n",
        "Classification Layer"
      ],
      "metadata": {
        "id": "GJk6pzJ_eyh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),  # emb_size의 1차원 벡터로 projection\n",
        "            nn.LayerNorm(emb_size), \n",
        "            nn.Linear(emb_size, n_classes))"
      ],
      "metadata": {
        "id": "Hgz60llFex06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VIT"
      ],
      "metadata": {
        "id": "t29mgGGie037"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,     \n",
        "                in_channels: int = 3,\n",
        "                patch_size: int = 16,\n",
        "                emb_size: int = 768,\n",
        "                img_size: int = 224,\n",
        "                depth: int = 12,\n",
        "                n_classes: int = 1000,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n",
        "        "
      ],
      "metadata": {
        "id": "M_c6Q4Suex8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(ViT(), (3, 224, 224), device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRBKmmHme1nO",
        "outputId": "e4e39a58-4446-4751-8177-e446a4c620a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
            "         Rearrange-2             [-1, 196, 768]               0\n",
            "    PatchEmbedding-3             [-1, 197, 768]               0\n",
            "         LayerNorm-4             [-1, 197, 768]           1,536\n",
            "            Linear-5            [-1, 197, 2304]       1,771,776\n",
            "           Dropout-6          [-1, 8, 197, 197]               0\n",
            "            Linear-7             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-8             [-1, 197, 768]               0\n",
            "           Dropout-9             [-1, 197, 768]               0\n",
            "      ResidualAdd-10             [-1, 197, 768]               0\n",
            "        LayerNorm-11             [-1, 197, 768]           1,536\n",
            "           Linear-12            [-1, 197, 3072]       2,362,368\n",
            "             GELU-13            [-1, 197, 3072]               0\n",
            "          Dropout-14            [-1, 197, 3072]               0\n",
            "           Linear-15             [-1, 197, 768]       2,360,064\n",
            "          Dropout-16             [-1, 197, 768]               0\n",
            "      ResidualAdd-17             [-1, 197, 768]               0\n",
            "        LayerNorm-18             [-1, 197, 768]           1,536\n",
            "           Linear-19            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-20          [-1, 8, 197, 197]               0\n",
            "           Linear-21             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-22             [-1, 197, 768]               0\n",
            "          Dropout-23             [-1, 197, 768]               0\n",
            "      ResidualAdd-24             [-1, 197, 768]               0\n",
            "        LayerNorm-25             [-1, 197, 768]           1,536\n",
            "           Linear-26            [-1, 197, 3072]       2,362,368\n",
            "             GELU-27            [-1, 197, 3072]               0\n",
            "          Dropout-28            [-1, 197, 3072]               0\n",
            "           Linear-29             [-1, 197, 768]       2,360,064\n",
            "          Dropout-30             [-1, 197, 768]               0\n",
            "      ResidualAdd-31             [-1, 197, 768]               0\n",
            "        LayerNorm-32             [-1, 197, 768]           1,536\n",
            "           Linear-33            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-34          [-1, 8, 197, 197]               0\n",
            "           Linear-35             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-36             [-1, 197, 768]               0\n",
            "          Dropout-37             [-1, 197, 768]               0\n",
            "      ResidualAdd-38             [-1, 197, 768]               0\n",
            "        LayerNorm-39             [-1, 197, 768]           1,536\n",
            "           Linear-40            [-1, 197, 3072]       2,362,368\n",
            "             GELU-41            [-1, 197, 3072]               0\n",
            "          Dropout-42            [-1, 197, 3072]               0\n",
            "           Linear-43             [-1, 197, 768]       2,360,064\n",
            "          Dropout-44             [-1, 197, 768]               0\n",
            "      ResidualAdd-45             [-1, 197, 768]               0\n",
            "        LayerNorm-46             [-1, 197, 768]           1,536\n",
            "           Linear-47            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-48          [-1, 8, 197, 197]               0\n",
            "           Linear-49             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-50             [-1, 197, 768]               0\n",
            "          Dropout-51             [-1, 197, 768]               0\n",
            "      ResidualAdd-52             [-1, 197, 768]               0\n",
            "        LayerNorm-53             [-1, 197, 768]           1,536\n",
            "           Linear-54            [-1, 197, 3072]       2,362,368\n",
            "             GELU-55            [-1, 197, 3072]               0\n",
            "          Dropout-56            [-1, 197, 3072]               0\n",
            "           Linear-57             [-1, 197, 768]       2,360,064\n",
            "          Dropout-58             [-1, 197, 768]               0\n",
            "      ResidualAdd-59             [-1, 197, 768]               0\n",
            "        LayerNorm-60             [-1, 197, 768]           1,536\n",
            "           Linear-61            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-62          [-1, 8, 197, 197]               0\n",
            "           Linear-63             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-64             [-1, 197, 768]               0\n",
            "          Dropout-65             [-1, 197, 768]               0\n",
            "      ResidualAdd-66             [-1, 197, 768]               0\n",
            "        LayerNorm-67             [-1, 197, 768]           1,536\n",
            "           Linear-68            [-1, 197, 3072]       2,362,368\n",
            "             GELU-69            [-1, 197, 3072]               0\n",
            "          Dropout-70            [-1, 197, 3072]               0\n",
            "           Linear-71             [-1, 197, 768]       2,360,064\n",
            "          Dropout-72             [-1, 197, 768]               0\n",
            "      ResidualAdd-73             [-1, 197, 768]               0\n",
            "        LayerNorm-74             [-1, 197, 768]           1,536\n",
            "           Linear-75            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-76          [-1, 8, 197, 197]               0\n",
            "           Linear-77             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-78             [-1, 197, 768]               0\n",
            "          Dropout-79             [-1, 197, 768]               0\n",
            "      ResidualAdd-80             [-1, 197, 768]               0\n",
            "        LayerNorm-81             [-1, 197, 768]           1,536\n",
            "           Linear-82            [-1, 197, 3072]       2,362,368\n",
            "             GELU-83            [-1, 197, 3072]               0\n",
            "          Dropout-84            [-1, 197, 3072]               0\n",
            "           Linear-85             [-1, 197, 768]       2,360,064\n",
            "          Dropout-86             [-1, 197, 768]               0\n",
            "      ResidualAdd-87             [-1, 197, 768]               0\n",
            "        LayerNorm-88             [-1, 197, 768]           1,536\n",
            "           Linear-89            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-90          [-1, 8, 197, 197]               0\n",
            "           Linear-91             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-92             [-1, 197, 768]               0\n",
            "          Dropout-93             [-1, 197, 768]               0\n",
            "      ResidualAdd-94             [-1, 197, 768]               0\n",
            "        LayerNorm-95             [-1, 197, 768]           1,536\n",
            "           Linear-96            [-1, 197, 3072]       2,362,368\n",
            "             GELU-97            [-1, 197, 3072]               0\n",
            "          Dropout-98            [-1, 197, 3072]               0\n",
            "           Linear-99             [-1, 197, 768]       2,360,064\n",
            "         Dropout-100             [-1, 197, 768]               0\n",
            "     ResidualAdd-101             [-1, 197, 768]               0\n",
            "       LayerNorm-102             [-1, 197, 768]           1,536\n",
            "          Linear-103            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-104          [-1, 8, 197, 197]               0\n",
            "          Linear-105             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-106             [-1, 197, 768]               0\n",
            "         Dropout-107             [-1, 197, 768]               0\n",
            "     ResidualAdd-108             [-1, 197, 768]               0\n",
            "       LayerNorm-109             [-1, 197, 768]           1,536\n",
            "          Linear-110            [-1, 197, 3072]       2,362,368\n",
            "            GELU-111            [-1, 197, 3072]               0\n",
            "         Dropout-112            [-1, 197, 3072]               0\n",
            "          Linear-113             [-1, 197, 768]       2,360,064\n",
            "         Dropout-114             [-1, 197, 768]               0\n",
            "     ResidualAdd-115             [-1, 197, 768]               0\n",
            "       LayerNorm-116             [-1, 197, 768]           1,536\n",
            "          Linear-117            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-118          [-1, 8, 197, 197]               0\n",
            "          Linear-119             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-120             [-1, 197, 768]               0\n",
            "         Dropout-121             [-1, 197, 768]               0\n",
            "     ResidualAdd-122             [-1, 197, 768]               0\n",
            "       LayerNorm-123             [-1, 197, 768]           1,536\n",
            "          Linear-124            [-1, 197, 3072]       2,362,368\n",
            "            GELU-125            [-1, 197, 3072]               0\n",
            "         Dropout-126            [-1, 197, 3072]               0\n",
            "          Linear-127             [-1, 197, 768]       2,360,064\n",
            "         Dropout-128             [-1, 197, 768]               0\n",
            "     ResidualAdd-129             [-1, 197, 768]               0\n",
            "       LayerNorm-130             [-1, 197, 768]           1,536\n",
            "          Linear-131            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-132          [-1, 8, 197, 197]               0\n",
            "          Linear-133             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-134             [-1, 197, 768]               0\n",
            "         Dropout-135             [-1, 197, 768]               0\n",
            "     ResidualAdd-136             [-1, 197, 768]               0\n",
            "       LayerNorm-137             [-1, 197, 768]           1,536\n",
            "          Linear-138            [-1, 197, 3072]       2,362,368\n",
            "            GELU-139            [-1, 197, 3072]               0\n",
            "         Dropout-140            [-1, 197, 3072]               0\n",
            "          Linear-141             [-1, 197, 768]       2,360,064\n",
            "         Dropout-142             [-1, 197, 768]               0\n",
            "     ResidualAdd-143             [-1, 197, 768]               0\n",
            "       LayerNorm-144             [-1, 197, 768]           1,536\n",
            "          Linear-145            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-146          [-1, 8, 197, 197]               0\n",
            "          Linear-147             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-148             [-1, 197, 768]               0\n",
            "         Dropout-149             [-1, 197, 768]               0\n",
            "     ResidualAdd-150             [-1, 197, 768]               0\n",
            "       LayerNorm-151             [-1, 197, 768]           1,536\n",
            "          Linear-152            [-1, 197, 3072]       2,362,368\n",
            "            GELU-153            [-1, 197, 3072]               0\n",
            "         Dropout-154            [-1, 197, 3072]               0\n",
            "          Linear-155             [-1, 197, 768]       2,360,064\n",
            "         Dropout-156             [-1, 197, 768]               0\n",
            "     ResidualAdd-157             [-1, 197, 768]               0\n",
            "       LayerNorm-158             [-1, 197, 768]           1,536\n",
            "          Linear-159            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-160          [-1, 8, 197, 197]               0\n",
            "          Linear-161             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-162             [-1, 197, 768]               0\n",
            "         Dropout-163             [-1, 197, 768]               0\n",
            "     ResidualAdd-164             [-1, 197, 768]               0\n",
            "       LayerNorm-165             [-1, 197, 768]           1,536\n",
            "          Linear-166            [-1, 197, 3072]       2,362,368\n",
            "            GELU-167            [-1, 197, 3072]               0\n",
            "         Dropout-168            [-1, 197, 3072]               0\n",
            "          Linear-169             [-1, 197, 768]       2,360,064\n",
            "         Dropout-170             [-1, 197, 768]               0\n",
            "     ResidualAdd-171             [-1, 197, 768]               0\n",
            "          Reduce-172                  [-1, 768]               0\n",
            "       LayerNorm-173                  [-1, 768]           1,536\n",
            "          Linear-174                 [-1, 1000]         769,000\n",
            "================================================================\n",
            "Total params: 86,415,592\n",
            "Trainable params: 86,415,592\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 364.33\n",
            "Params size (MB): 329.65\n",
            "Estimated Total Size (MB): 694.56\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dBUQP5bLgo_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}