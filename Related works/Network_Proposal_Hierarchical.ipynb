{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_Proposal_Hierarchical.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IuksMeF5auhd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "import os\n",
        "from STN import Stn, Stn11, Stn2_1, Stn2_2, Stn2_3\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                     stride=1, padding=1, bias=True)\n",
        "\n",
        "def conv2x2(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=2,\n",
        "                     stride=2, padding=0, bias=True)\n",
        "\n",
        "def conv1x1(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                     stride=1, padding=0, bias=True)"
      ],
      "metadata": {
        "id": "zNobcZGza8jZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder1(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Encoder1, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # 1 - Ori Size\n",
        "        self.conv3_1_1 = conv3x3(in_channels=self.in_channels, out_channels=16)\n",
        "        self.relu1_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_2 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_3 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 2 - 1/2 Size\n",
        "        self.conv2_2_1 = conv2x2(in_channels=16, out_channels=32)\n",
        "        self.relu2_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_1 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_2 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 3 - 1/4 Size\n",
        "        self.conv2_3_1 = conv2x2(in_channels=32, out_channels=64)\n",
        "        self.relu3_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        self.conv2_4_1 = conv2x2(in_channels=64, out_channels=128)\n",
        "        self.relu4_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_1 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_2 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1 - Ori Size\n",
        "        # 1 채널로 바꾸기\n",
        "        # x = x[:, 1, :, :]\n",
        "        # x = x.unsqueeze(1)\n",
        "        y1_1 = self.conv3_1_1(x)\n",
        "        y1_1 = self.relu1_1(y1_1)\n",
        "        y1_2 = self.conv3_1_2(y1_1)\n",
        "        y1_2 = self.relu1_2(y1_2)\n",
        "        y1_3 = self.conv3_1_3(y1_2)\n",
        "        y1_3 = self.relu1_3(y1_3)  # 32 channel\n",
        "\n",
        "        # 2 - 1/2 Size\n",
        "        y2_1 = self.conv2_2_1(y1_3)  # size는 감소, 채널 증가(16->32)\n",
        "        y2_1 = self.relu2_1(y2_1)\n",
        "        y2_2 = self.conv3_2_1(y2_1)\n",
        "        y2_2 = self.relu2_2(y2_2)\n",
        "        y2_3 = self.conv3_2_2(y2_2)\n",
        "        out1 = self.relu2_3(y2_3)\n",
        "\n",
        "        # 3 - 1/4 Size\n",
        "        y3_1 = self.conv2_3_1(out1)  # size는 감소, 채널 증가(32->64)\n",
        "        y3_1 = self.relu3_1(y3_1)\n",
        "        y3_2 = self.conv3_3_1(y3_1)\n",
        "        y3_2 = self.relu3_2(y3_2)\n",
        "        y3_3 = self.conv3_3_2(y3_2)\n",
        "        y3_3 = self.relu3_3(y3_3)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        y4_1 = self.conv2_4_1(y3_3)  # size는 감소, 채널 증가(64->128)\n",
        "        y4_1 = self.relu4_1(y4_1)\n",
        "        y4_2 = self.conv3_4_1(y4_1)\n",
        "        y4_2 = self.relu4_2(y4_2)\n",
        "        y4_3 = self.conv3_4_2(y4_2)\n",
        "        out2 = self.relu4_3(y4_3)\n",
        "        return out1, out2  # 1/2 size with 32 channels, 1/8 size with 128 channels"
      ],
      "metadata": {
        "id": "0tR5gCAgbVuT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder2(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Encoder2, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # 1 - Ori Size\n",
        "        self.conv3_1_1 = conv3x3(in_channels=self.in_channels, out_channels=16)\n",
        "        self.relu1_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_2 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_1_3 = conv3x3(in_channels=16, out_channels=16)\n",
        "        self.relu1_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 2 - 1/2 Size\n",
        "        self.conv2_2_1 = conv2x2(in_channels=16, out_channels=32)\n",
        "        self.relu2_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_1 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_2_2 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu2_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 3 - 1/4 Size\n",
        "        self.conv2_3_1 = conv2x2(in_channels=32, out_channels=64)\n",
        "        self.relu3_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_3_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu3_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        self.conv2_4_1 = conv2x2(in_channels=64, out_channels=128)\n",
        "        self.relu4_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_1 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_2 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_3 = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1 - Ori Size\n",
        "        y1_1 = self.conv3_1_1(x)\n",
        "        y1_1 = self.relu1_1(y1_1)\n",
        "        y1_2 = self.conv3_1_2(y1_1)\n",
        "        y1_2 = self.relu1_2(y1_2)\n",
        "        y1_3 = self.conv3_1_3(y1_2)\n",
        "        y1_3 = self.relu1_3(y1_3)\n",
        "        out1 = y1_3  # 16 channels\n",
        "\n",
        "        # 2 - 1/2 Size\n",
        "        y2_1 = self.conv2_2_1(y1_3)\n",
        "        y2_1 = self.relu2_1(y2_1)\n",
        "        y2_2 = self.conv3_2_1(y2_1)\n",
        "        y2_2 = self.relu2_2(y2_2)\n",
        "        y2_3 = self.conv3_2_2(y2_2)\n",
        "        y2_3 = self.relu2_3(y2_3)\n",
        "        out2 = y2_3  # 32 channels\n",
        "\n",
        "        # 3 - 1/4 Size\n",
        "        y3_1 = self.conv2_3_1(y2_3)\n",
        "        y3_1 = self.relu3_1(y3_1)\n",
        "        y3_2 = self.conv3_3_1(y3_1)\n",
        "        y3_2 = self.relu3_2(y3_2)\n",
        "        y3_3 = self.conv3_3_2(y3_2)\n",
        "        y3_3 = self.relu3_3(y3_3)\n",
        "        out3 = y3_3  # 64 channels\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        y4_1 = self.conv2_4_1(y3_3)\n",
        "        y4_1 = self.relu4_1(y4_1)\n",
        "        y4_2 = self.conv3_4_1(y4_1)\n",
        "        y4_2 = self.relu4_2(y4_2)\n",
        "        y4_3 = self.conv3_4_2(y4_2)\n",
        "        y4_3 = self.relu4_3(y4_3)\n",
        "        out4 = y4_3  # 128 channels\n",
        "        return out1, out2, out3, out4  # stage1 ~4"
      ],
      "metadata": {
        "id": "EMY7XOLjbbDL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder1(nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super(Decoder1, self).__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.PS = nn.PixelShuffle(2)  # (B, C x 2^2, H, W) -> (B, C, H x 2, W x 2)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        self.conv3_4_3 = conv3x3(in_channels=512, out_channels=512)\n",
        "        self.relu4_6 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_4 = conv3x3(in_channels=512, out_channels=1024)\n",
        "        self.relu4_7 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 5 - 1/4 Size\n",
        "        self.conv3_5_1 = conv3x3(in_channels=256, out_channels=256)\n",
        "        self.relu5_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_5_2 = conv3x3(in_channels=256, out_channels=512)\n",
        "        self.relu5_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 6 - 1/2 Size\n",
        "        self.conv3_6_1 = conv3x3(in_channels=256, out_channels=256)  # 128 + 128\n",
        "        self.relu6_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_6_2 = conv3x3(in_channels=256, out_channels=256)\n",
        "        self.relu6_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 7 - Ori Size\n",
        "        self.conv3_7_1 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu7_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_7_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu7_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv1_7_1 = conv1x1(in_channels=64, out_channels=self.out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):  # 128, 512 channels\n",
        "        # 4 - 1/8 Size\n",
        "        y4_6 = self.conv3_4_3(x2)\n",
        "        y4_6 = self.relu4_6(y4_6)\n",
        "        y4_7 = self.conv3_4_4(y4_6)\n",
        "        y4_7 = self.relu4_7(y4_7)  # 512 channels\n",
        "\n",
        "        # 5 - 1/4 Size\n",
        "        y5_1 = self.PS(y4_7)  # 128 channels\n",
        "        y5_2 = self.conv3_5_1(y5_1)\n",
        "        y5_2 = self.relu5_1(y5_2)\n",
        "        y5_3 = self.conv3_5_2(y5_2)\n",
        "        y5_3 = self.relu5_2(y5_3)\n",
        "\n",
        "        # 6 - 1/2 Size\n",
        "        y6_1 = self.PS(y5_3)\n",
        "        y6_2 = torch.cat((x1, y6_1), 1)\n",
        "        y6_3 = self.conv3_6_1(y6_2)\n",
        "        y6_3 = self.relu6_1(y6_3)\n",
        "        y6_4 = self.conv3_6_2(y6_3)\n",
        "        y6_4 = self.relu6_2(y6_4)\n",
        "\n",
        "        # 7 - Ori Size\n",
        "        y7_1 = self.PS(y6_4)\n",
        "        y7_2 = self.conv3_7_1(y7_1)\n",
        "        y7_2 = self.relu7_1(y7_2)\n",
        "        y7_3 = self.conv3_7_2(y7_2)\n",
        "        y7_3 = self.relu7_2(y7_3)\n",
        "        out = self.conv1_7_1(y7_3)\n",
        "        return out"
      ],
      "metadata": {
        "id": "mgHSmHJVbeIJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder2(nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super(Decoder2, self).__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.PS = nn.PixelShuffle(2)\n",
        "\n",
        "        # 4 - 1/8 Size\n",
        "        self.conv3_4_3 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu4_6 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_4_4 = conv3x3(in_channels=128, out_channels=256)\n",
        "        self.relu4_7 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 5 - 1/4 Size\n",
        "        self.conv3_5_1 = conv3x3(in_channels=128, out_channels=128)  # 64+64 (Concat)\n",
        "        self.relu5_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_5_2 = conv3x3(in_channels=128, out_channels=128)\n",
        "        self.relu5_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 6 - 1/2 Size\n",
        "        self.conv3_6_1 = conv3x3(in_channels=64, out_channels=64)  # 32+32 (Concat)\n",
        "        self.relu6_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_6_2 = conv3x3(in_channels=64, out_channels=64)\n",
        "        self.relu6_2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        # 7 - Ori Size\n",
        "        self.conv3_7_1 = conv3x3(in_channels=32, out_channels=32)  # 16+16 (Concat)\n",
        "        self.relu7_1 = nn.LeakyReLU(0.1)\n",
        "        self.conv3_7_2 = conv3x3(in_channels=32, out_channels=32)\n",
        "        self.relu7_2 = nn.LeakyReLU(0.1)\n",
        "        self.conv1_7_1 = conv1x1(in_channels=32, out_channels=self.out_channels)\n",
        "\n",
        "    def forward(self, x1, x2, x3, x4):  # 16, 32, 64, 128 channels\n",
        "        # 4 - 1/8 Size with 128 channels\n",
        "        y4_1 = self.conv3_4_3(x4)\n",
        "        y4_1 = self.relu4_6(y4_1)\n",
        "        y4_2 = self.conv3_4_4(y4_1)  # 256 channels\n",
        "        y4_2 = self.relu4_7(y4_2)\n",
        "\n",
        "        # 5 - 1/4 Size with 64 channels\n",
        "        y5_1 = self.PS(y4_2)  # 256 -> 64 channels\n",
        "        y5_2 = torch.cat((x3, y5_1), 1)  # Concat -> 128 channels\n",
        "        y5_3 = self.conv3_5_1(y5_2)\n",
        "        y5_3 = self.relu5_1(y5_3)\n",
        "        y5_4 = self.conv3_5_2(y5_3)\n",
        "        y5_4 = self.relu5_2(y5_4)\n",
        "\n",
        "        # 6 - 1/2 Size with 32 channels\n",
        "        y6_1 = self.PS(y5_4)  # 128 -> 32 channels\n",
        "        y6_2 = torch.cat((x2, y6_1), 1)  # Concat -> 64 channels\n",
        "        y6_3 = self.conv3_6_1(y6_2)\n",
        "        y6_3 = self.relu6_1(y6_3)\n",
        "        y6_4 = self.conv3_6_2(y6_3)\n",
        "        y6_4 = self.relu6_2(y6_4)\n",
        "\n",
        "        # 7 - Ori Size\n",
        "        y7_1 = self.PS(y6_4)  # 64 -> 16 channels\n",
        "        y7_2 = torch.cat((x1, y7_1), 1)  # Concat -> 32 channels\n",
        "        y7_3 = self.conv3_7_1(y7_2)\n",
        "        y7_3 = self.relu7_1(y7_3)\n",
        "        y7_4 = self.conv3_7_2(y7_3)\n",
        "        y7_4 = self.relu7_2(y7_4)\n",
        "        out = self.conv1_7_1(y7_4)  # 32 -> 3 channels\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "G_Gcf2FgbkoW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from base_networks import *\n",
        "from torchvision.transforms import *\n",
        "import torch.nn.functional as F\n",
        "from ConvLSTM import ConvLSTMCell\n",
        "import numpy as np\n",
        "# from vit_pytorch import ViT\n",
        "\n",
        "class Stn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn, self).__init__()\n",
        "\n",
        "        # self.v = ViT(\n",
        "        #     image_size=128,\n",
        "        #     patch_size=16,\n",
        "        #     num_classes=1000,\n",
        "        #     dim=1024,\n",
        "        #     depth=6,\n",
        "        #     heads=16,\n",
        "        #     mlp_dim = 2048,\n",
        "        #     dropout=0.1,\n",
        "        #     emb_dropout=0.1)\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(1, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(64000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([0.9, 0, 0, 0, 0.9, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 64000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta,x.size(),align_corners=True)\n",
        "        out = F.grid_sample(x, grid,align_corners=True)\n",
        "        return out\n",
        "\n",
        "class Stn11(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn11, self).__init__()\n",
        "\n",
        "        # self.v = ViT(\n",
        "        #     image_size=128,\n",
        "        #     patch_size=16,\n",
        "        #     num_classes=1000,\n",
        "        #     dim=1024,\n",
        "        #     depth=6,\n",
        "        #     heads=16,\n",
        "        #     mlp_dim = 2048,\n",
        "        #     dropout=0.1,\n",
        "        #     emb_dropout=0.1)\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(3, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(64000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([0.9, 0, 0, 0, 0.9, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 64000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta,x.size(),align_corners=True)\n",
        "        out = F.grid_sample(x, grid,align_corners=True)\n",
        "        return out\n",
        "\n",
        "class Stn2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn2, self).__init__()\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(3, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(784000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 784000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        out = F.grid_sample(x, grid)\n",
        "        return out, theta\n",
        "\n",
        "\n",
        "class Stn2_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn2_1, self).__init__()\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(1, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(16000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([0.9, 0, 0, 0, 0.9, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 16000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        out = F.grid_sample(x, grid)\n",
        "        return out\n",
        "\n",
        "class Stn2_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn2_2, self).__init__()\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(1, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(32000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([0.9, 0, 0, 0, 0.9, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 32000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        out = F.grid_sample(x, grid)\n",
        "        return out\n",
        "\n",
        "class Stn2_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stn2_3, self).__init__()\n",
        "\n",
        "        self.st = nn.Sequential(\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(1, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(250, 250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "        self.FC_ = nn.Sequential(\n",
        "            nn.Linear(1000, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        self.FC_[2].weight.data.zero_()\n",
        "        self.FC_[2].bias.data.copy_(torch.tensor([0.9, 0, 0, 0, 0.9, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.st(x)\n",
        "        h = h.view(-1, 1000)\n",
        "        h = self.FC_(h)\n",
        "        theta = h.view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        out = F.grid_sample(x, grid)\n",
        "        return out, theta\n",
        "\n",
        "\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FC, self).__init__()\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(3, 6),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(6, 3),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(3, 1)\n",
        "        )\n",
        "        self.FC[0].weight.data.zero_()\n",
        "        self.FC[0].bias.data.copy_(torch.tensor([1], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.FC(x)\n",
        "        return h"
      ],
      "metadata": {
        "id": "RWElNG_rbpx6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def partition4(x):  # X : (B, C, H, W) 가정\n",
        "    H, W = x.shape[2], x.shape[3]\n",
        "    pivot_H, pivot_W = H // 2, W // 2\n",
        "\n",
        "    left_upper = x[:, :, 0:pivot_H, 0:pivot_W]  # 좌상단\n",
        "    right_upper = x[:, :, 0:pivot_H, pivot_W:]  # 우상단\n",
        "    left_lower = x[:, :, pivot_H:, 0:pivot_W]  # 좌하단\n",
        "    right_lower = x[:, :, pivot_H:, pivot_W:]  # 우하단 \n",
        "\n",
        "    return left_upper, right_upper, left_lower, right_lower\n",
        "    \n",
        "# 좌우 2분할 : (B, C, H, W) -> (B, C, H, 2/W) for each patch\n",
        "def partition2_vertical(x):  \n",
        "    H, W = x.shape[2], x.shape[3]\n",
        "    pivot = W // 2\n",
        "\n",
        "    left = x[:, :, :, 0:pivot]\n",
        "    right = x[:, :, :, pivot:]\n",
        "\n",
        "    return left, right\n",
        "    \n",
        "# 상하 2분할 : (B, C, H, W) -> (B, C, 2/H, W) for each patch\n",
        "def partition2_horizontal(x):\n",
        "    H, W = x.shape[2], x.shape[3]\n",
        "    pivot = W // 2\n",
        "\n",
        "    upper = x[:, :, 0:pivot, :]\n",
        "    lower = x[:, :, pivot:, :]\n",
        "\n",
        "    return upper, lower"
      ],
      "metadata": {
        "id": "HQpg1oDRijHP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward"
      ],
      "metadata": {
        "id": "tBCqzJ8HiZQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_lv1 = models.Encoder()  # Original\n",
        "encoder_lv2 = models.Encoder()  # 상하 2분할\n",
        "encoder_lv3 = models.Encoder()  # 4분할\n",
        "\n",
        "decoder_lv1 = models.Decoder()\n",
        "decoder_lv2 = models.Decoder()\n",
        "decoder_lv3 = models.Decoder()"
      ],
      "metadata": {
        "id": "_QUnWRPjiWKp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stn4_1 = Stn2_1()  # theta : 16000 -> 250 -> 6\n",
        "stn4_2 = Stn2_1()\n",
        "stn4_3 = Stn2_1()\n",
        "stn4_4 = Stn2_1()\n",
        "\n",
        "stn2_1 = Stn2_2()  # theta : 32000 -> 250 -> 6\n",
        "stn2_2 = Stn2_2()\n",
        "\n",
        "stn1_1 = Stn()\n",
        "stn_rgb = Stn11()\n",
        "\n",
        "conv5_1 = conv3x3(in_channels=16, out_channels=16)\n",
        "conv5_2 = conv3x3(in_channels=32, out_channels=32)\n",
        "conv5_3 = conv3x3(in_channels=64, out_channels=64)\n",
        "conv5_4 = conv3x3(in_channels=128, out_channels=128)\n",
        "\n",
        "E5 = Encoder2(3)  # Input : RGB 3 channels\n",
        "E6 = Encoder1(1)  # Input : Grayscale 1 channel\n",
        "D2 = Decoder2(3)  # Output : RGB 3 channels"
      ],
      "metadata": {
        "id": "LNmclzLebp2u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs"
      ],
      "metadata": {
        "id": "i5Gqg1pmi18T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x4 = torch.randn(1, 1, 128, 128)  # grayscale\n",
        "\n",
        "shape1 = x4  # 원본\n",
        "upper, lower = partition2_horizontal(shape1)  # 상하 2 분할\n",
        "lu, ru, ll, rl = partition4(shape1)  # 4 분할\n",
        "\n",
        "print(shape1.shape)\n",
        "print(upper.shape, lower.shape)\n",
        "print(lu.shape, ru.shape, ll.shape, rl.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97AQ2EpphWzB",
        "outputId": "f1e61371-f9ef-4cd2-9a4b-480c913a5cfe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 128, 128])\n",
            "torch.Size([1, 1, 64, 128]) torch.Size([1, 1, 64, 128])\n",
            "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STN Outputs"
      ],
      "metadata": {
        "id": "_SQA_cibkMQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape1 = stn1_1(shape1)\n",
        "print('shape1 :', shape1.shape)\n",
        "print()\n",
        "\n",
        "shape21 = stn2_1(upper) \n",
        "shape22 = stn2_2(lower)\n",
        "print('shape21 :', shape21.shape)\n",
        "print('shape22 :', shape22.shape)\n",
        "print()\n",
        "\n",
        "shape41 = stn4_1(lu)\n",
        "shape42 = stn4_2(ru)\n",
        "shape43 = stn4_3(ll)\n",
        "shape44 = stn4_4(rl)\n",
        "print('shape41 :', shape41.shape)\n",
        "print('shape42 :', shape42.shape)\n",
        "print('shape43 :', shape43.shape)\n",
        "print('shape44 :', shape44.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubquwR-di1X6",
        "outputId": "450f238f-e029-4f25-d820-10408976193f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape1 : torch.Size([1, 1, 128, 128])\n",
            "\n",
            "shape21 : torch.Size([1, 1, 64, 128])\n",
            "shape22 : torch.Size([1, 1, 64, 128])\n",
            "\n",
            "shape41 : torch.Size([1, 1, 64, 64])\n",
            "shape42 : torch.Size([1, 1, 64, 64])\n",
            "shape43 : torch.Size([1, 1, 64, 64])\n",
            "shape44 : torch.Size([1, 1, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4066: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding & Decoding 1  \n",
        "\n",
        "4 분할 입력의 STN outputs 처리"
      ],
      "metadata": {
        "id": "MCl7IxmnmVcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_lv41 = encoder_lv3(shape41)\n",
        "feature_lv42 = encoder_lv3(shape42)\n",
        "feature_lv43 = encoder_lv3(shape43)\n",
        "feature_lv44 = encoder_lv3(shape44)\n",
        "\n",
        "print('feature_lv41 :', feature_lv41.shape)\n",
        "print('feature_lv42 :', feature_lv42.shape)\n",
        "print('feature_lv43 :', feature_lv43.shape)\n",
        "print('feature_lv44 :', feature_lv44.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfgOmSk4i1ag",
        "outputId": "d177a38a-b3a8-4763-811f-dcea82e8483f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_lv41 : torch.Size([1, 128, 16, 16])\n",
            "feature_lv42 : torch.Size([1, 128, 16, 16])\n",
            "feature_lv43 : torch.Size([1, 128, 16, 16])\n",
            "feature_lv44 : torch.Size([1, 128, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_lv41 = decoder_lv3(feature_lv41)\n",
        "out_lv42 = decoder_lv3(feature_lv42)\n",
        "out_lv43 = decoder_lv3(feature_lv43)\n",
        "out_lv44 = decoder_lv3(feature_lv44)\n",
        "\n",
        "print('out_lv41 :', out_lv41.shape)\n",
        "print('out_lv42 :', out_lv42.shape)\n",
        "print('out_lv43 :', out_lv43.shape)\n",
        "print('out_lv44 :', out_lv44.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_28uTIMLlWOD",
        "outputId": "5d92f757-ce13-4531-d93d-412514d69812"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_lv41 : torch.Size([1, 1, 64, 64])\n",
            "out_lv42 : torch.Size([1, 1, 64, 64])\n",
            "out_lv43 : torch.Size([1, 1, 64, 64])\n",
            "out_lv44 : torch.Size([1, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다음 계층에 전달할 사항 정리\n",
        "\n",
        "2 분할 STN Outputs의 처리 계층의 encoder, decoder에 전달할 사항들"
      ],
      "metadata": {
        "id": "cq1s0Hiqo0Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음 계층 encoder output에 전달\n",
        "feature_lv412 = torch.cat([feature_lv41, feature_lv42], dim=-1)  # 좌상단, 우상단 concat\n",
        "feature_lv434 = torch.cat([feature_lv43, feature_lv44], dim=-1)  # 좌하단, 우하단 concat\n",
        "print('feature_lv412 :', feature_lv412.shape)  \n",
        "print('feature_lv434 :', feature_lv434.shape)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfV8hzlaozQh",
        "outputId": "64dc8612-39ff-4dec-9e0e-9b0d69e65974"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_lv412 : torch.Size([1, 128, 16, 32])\n",
            "feature_lv434 : torch.Size([1, 128, 16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음 계층 encoder input에 전달\n",
        "out_lv412 = torch.cat([out_lv41, out_lv42], dim=-1)  # 좌상단, 우상단 concat\n",
        "out_lv434 = torch.cat([out_lv43, out_lv44], dim=-1)  # 좌하단, 우하단 concat\n",
        "print('out_lv412 :', out_lv412.shape)  \n",
        "print('out_lv434 :', out_lv434.shape)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPWLizGvozVN",
        "outputId": "cd39f9e7-e7d3-483c-8c55-31cf60c2f306"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_lv412 : torch.Size([1, 1, 64, 128])\n",
            "out_lv434 : torch.Size([1, 1, 64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding & Decoding - 2\n",
        "\n",
        "2 분할 입력의 STN Outputs 처리\n",
        "\n",
        " 이전 계층의 입력을 전달 받음"
      ],
      "metadata": {
        "id": "U6jlCX_LnT6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전 계층의 decoder 출력 + 현재 계층의 입력(STN Outputs)\n",
        "shape21_ = shape21 + out_lv412\n",
        "print('shape21 :', shape21.shape)\n",
        "print('out_lv412 :', out_lv412.shape)\n",
        "print('shape21_ :', shape21_.shape)\n",
        "print()\n",
        "\n",
        "shape22_ = shape22 + out_lv434\n",
        "print('shape22 :', shape22.shape)\n",
        "print('out_lv434 :', out_lv434.shape)\n",
        "print('shape22_ :', shape22_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drsV5dcJq6Ty",
        "outputId": "2a3f820f-454f-4599-e1f4-be34cf300af0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape21 : torch.Size([1, 1, 64, 128])\n",
            "out_lv412 : torch.Size([1, 1, 64, 128])\n",
            "shape21_ : torch.Size([1, 1, 64, 128])\n",
            "\n",
            "shape22 : torch.Size([1, 1, 64, 128])\n",
            "out_lv434 : torch.Size([1, 1, 64, 128])\n",
            "shape22_ : torch.Size([1, 1, 64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder 통과\n",
        "feature_lv21 = encoder_lv2(shape21_)\n",
        "feature_lv22 = encoder_lv2(shape22_)\n",
        "\n",
        "# 이전 계층의 Encoder output을 전달 받음 -> Decoder 입력\n",
        "feature_lv21_ = feature_lv21 + feature_lv412\n",
        "feature_lv22_ = feature_lv22 + feature_lv434\n",
        "\n",
        "print('feature_lv21 :', feature_lv21.shape)\n",
        "print('feature_lv22 :', feature_lv22.shape)\n",
        "print('feature_lv21_ :', feature_lv21_.shape)\n",
        "print('feature_lv22_ :', feature_lv22_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1agos1cmlYS3",
        "outputId": "35d925ac-79a9-4219-ceb4-0b3440b300b4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_lv21 : torch.Size([1, 128, 16, 32])\n",
            "feature_lv22 : torch.Size([1, 128, 16, 32])\n",
            "feature_lv21_ : torch.Size([1, 128, 16, 32])\n",
            "feature_lv22_ : torch.Size([1, 128, 16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_lv21 = decoder_lv2(feature_lv21_)\n",
        "out_lv22 = decoder_lv2(feature_lv22_)\n",
        "\n",
        "print('out_lv21 :', out_lv21.shape)\n",
        "print('out_lv22 :', out_lv22.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRlCikUAnY8N",
        "outputId": "d61505d9-5457-48a1-f49d-153624592f18"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_lv21 : torch.Size([1, 1, 64, 128])\n",
            "out_lv22 : torch.Size([1, 1, 64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다음 계층에 전달할 사항 정리"
      ],
      "metadata": {
        "id": "1b0_cLAws4TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음 계층 encoder output에 전달\n",
        "feature_lv212 = torch.cat([feature_lv21_, feature_lv22_], dim=-2)\n",
        "print('feature_lv21 :', feature_lv21.shape)\n",
        "print('feature_lv22 :', feature_lv22.shape)\n",
        "print('feature_lv212 :', feature_lv212.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1NAeGghnY-o",
        "outputId": "bf1c7528-3cc6-4548-9144-3f3bf7892cf8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_lv21 : torch.Size([1, 128, 16, 32])\n",
            "feature_lv22 : torch.Size([1, 128, 16, 32])\n",
            "feature_lv212 : torch.Size([1, 128, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음 계층 encoder input에 전달\n",
        "out_lv212 = torch.cat([out_lv21, out_lv22], dim=-2)\n",
        "print('out_lv21 :', out_lv21.shape)\n",
        "print('out_lv22 :', out_lv22.shape)\n",
        "print('out_lv212 :', out_lv212.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPe8VFScnZCA",
        "outputId": "d3c054d0-30f5-4b22-ba14-d1a55a023174"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_lv21 : torch.Size([1, 1, 64, 128])\n",
            "out_lv22 : torch.Size([1, 1, 64, 128])\n",
            "out_lv212 : torch.Size([1, 1, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding & Decoding - 3\n",
        "\n",
        "원본 입력의 STN Outputs 처리\n",
        "\n",
        "이전 계층의 입력을 전달 받음"
      ],
      "metadata": {
        "id": "6novhJt9uzaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape1_ = shape1 + out_lv212\n",
        "print('shape1 :', shape1.shape)\n",
        "print('out_lv212 :', out_lv212.shape)\n",
        "print('shape1_ :', shape1_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHCLsiSgnZEZ",
        "outputId": "5db9cb03-8f07-4354-e675-ecb618c63a52"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape1 : torch.Size([1, 1, 128, 128])\n",
            "out_lv212 : torch.Size([1, 1, 128, 128])\n",
            "shape1_ : torch.Size([1, 1, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder 통과\n",
        "feature_lv1 = encoder_lv1(shape1_)\n",
        "\n",
        "# 이전 계층의 Encoder ouput을 전달 받음\n",
        "feature_lv1_ = feature_lv1 + feature_lv212\n",
        "print('feature_lv1 :', feature_lv1.shape)\n",
        "print('feature_lv212 :', feature_lv212.shape)\n",
        "print('feature_lv1_ :', feature_lv1_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af58go1mvCZA",
        "outputId": "6057a38f-3cb0-46e9-a7d6-5a28b4b7bef4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_lv1 : torch.Size([1, 128, 32, 32])\n",
            "feature_lv212 : torch.Size([1, 128, 32, 32])\n",
            "feature_lv1_ : torch.Size([1, 128, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_lv1 = decoder_lv1(feature_lv1_)  # S_t+1 hat (predicted gray-scale shape image - 1 channel)\n",
        "print(out_lv1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAdu_hrgvRWO",
        "outputId": "a889a314-a3b2-41b5-8c09-7f31f1d801d5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding & Decoding 4\n",
        "\n",
        "out_lv1을 입력으로 함\n",
        "\n",
        " RGB 이미지에 대한 stage1~4 encoder output을 전달 받음"
      ],
      "metadata": {
        "id": "2h8UqTQiwNCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding S_t+1 hat\n",
        "temp, y5_4 = E6(out_lv1)\n",
        "print('temp :', temp.shape)  # 1/2 size\n",
        "print('y5_4 :', y5_4.shape)  # 1/8 size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuyMmSvivRcx",
        "outputId": "d9faf10a-471e-4674-9042-3258a610351f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temp : torch.Size([1, 32, 64, 64])\n",
            "y5_4 : torch.Size([1, 128, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB 입력\n",
        "x5 = torch.randn(1, 3, 128, 128)  \n",
        "\n",
        "# STN\n",
        "x5_5 = stn_rgb(x5)  # STN - 3 channels\n",
        "\n",
        "# 4 stage convolutional encoding\n",
        "y5_1, y5_2, y5_3, y5_4 = E5(x5_5)  # 16, 32, 64, 128 channels (Outputs of 4 stages)\n",
        "\n",
        "print('x5 :', x5.shape)\n",
        "print('x5_5 :', x5_5.shape)\n",
        "print()\n",
        "print('y5_1 :', y5_1.shape)  # Original size with 16 channels \n",
        "print('y5_2 :', y5_2.shape)  # 1/2 size with 32 channels\n",
        "print('y5_3 :', y5_3.shape)  # 1/4 size with 64 channels\n",
        "print('y5_4 :', y5_4.shape)  # 1/8 size with 128 channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hUxTJ23wj0m",
        "outputId": "a1eb9b5f-d81b-4dab-ccd4-3f17c49764e9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x5 : torch.Size([1, 3, 128, 128])\n",
            "x5_5 : torch.Size([1, 3, 128, 128])\n",
            "\n",
            "y5_1 : torch.Size([1, 16, 128, 128])\n",
            "y5_2 : torch.Size([1, 32, 64, 64])\n",
            "y5_3 : torch.Size([1, 64, 32, 32])\n",
            "y5_4 : torch.Size([1, 128, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding 4 stage encoded outputs\n",
        "D2_in1 = conv5_1(y5_1)  # 16 channels\n",
        "D2_in2 = conv5_2(y5_2)  # 32 channels\n",
        "D2_in3 = conv5_3(y5_3)  # 64 channels\n",
        "D2_in4 = conv5_4(y5_4)  # 128 channels\n",
        "\n",
        "print('D2_in1 :', D2_in1.shape)\n",
        "print('D2_in2 :', D2_in2.shape)\n",
        "print('D2_in3 :', D2_in3.shape)\n",
        "print('D2_in4 :', D2_in4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0J-NXNCxDRs",
        "outputId": "3de70e96-0e12-43c9-9d8e-936d596b2d8e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D2_in1 : torch.Size([1, 16, 128, 128])\n",
            "D2_in2 : torch.Size([1, 32, 64, 64])\n",
            "D2_in3 : torch.Size([1, 64, 32, 32])\n",
            "D2_in4 : torch.Size([1, 128, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = D2(D2_in1, D2_in2, D2_in3, D2_in4)  \n",
        "print('out2 :', out2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMf_vZj7x9iR",
        "outputId": "43eb15a2-9f0c-4372-984b-f229eb190dd3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out2 : torch.Size([1, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n_UGK2feyYv-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}